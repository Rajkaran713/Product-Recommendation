# -*- coding: utf-8 -*-
"""seg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b5QVk6IHJATDgeMoJvJjuuCG48Zy_La2
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from nltk.corpus import stopwords
import string
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
import scipy.stats as stats

df = pd.read_csv('/content/sample_data/Book1 (1).csv')
df.head()

df.info()

df.describe()

from datetime import datetime
df['date'] = df['date'].apply(lambda x : datetime.strptime(x, '%m/%d/%Y'))

df['positivity'] = df['rating'].apply(lambda x : 1.0 if x > 3.0 else -1.0)

import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use("ggplot")
# Brand distribution
ax = df.groupby("brand").count()["asin"].plot(kind="pie",
                                                 figsize=(8, 8),
                                                 title="Number of Offerings grouped by Brand")
plt.show()

df.groupby('brand').count()['rating'].sort_values(ascending=False)

ax = sns.countplot(data = df, x ='brand')
ax.set_title('Frequency Distribution of brand', fontsize=20)
for p in ax.patches:
        ax.annotate('{:}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))

import seaborn as sns
sns.pointplot(y="brand", x="rating", data=df)

corelation = df.corr()
corelation

plt.figure(figsize=(10,5))
ax=sns.heatmap(df.corr(),annot=True)

df.groupby('brand').mean()['rating'].sort_values()

print("The dataset contains {0[0]: ,.0f} rows and {0[1]: .0f} variables.".format(df.shape))

ax = pd.pivot_table(df,
                    index="date",
                    columns="brand",
                    values="asin",
                    aggfunc="count",
                    fill_value=0).plot.area(title="Monthly Number of Reviews per Brand", figsize=(10, 6))

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords
import string
stop = set(stopwords.words('english'))
punc = set(string.punctuation)
keywords = df["brand"].apply(lambda x: x.lower()).unique().tolist()
keywords.append("phone")
lemma = WordNetLemmatizer()
def clean_text(text):
    # Convert the text into lowercase
    text = text.lower()
    # Split into list
    wordList = text.split()
    # Remove punctuation
    wordList = ["".join(x for x in word if (x=="'")|(x not in punc)) for word in wordList]
    # Remove stopwords
    wordList = [word for word in wordList if word not in stop]
    # Remove other keywords
    wordList = [word for word in wordList if word not in keywords]
    # Lemmatisation
    wordList = [lemma.lemmatize(word) for word in wordList]
    return " ".join(wordList)
clean_text("I love phone.")

df["subject"] = df["subject"].astype("str")
df["clean_text"] = df["subject"].apply(clean_text)

df["clean_text"].head().values

def word_freq_dict(text):
    # Convert text into word list
    wordList = text.split()
    # Generate word freq dictionary
    wordFreqDict = {word: wordList.count(word) for word in wordList}
    return wordFreqDict
word_freq_dict("I love this phone. I love it.")

apple = df[df["brand"]=="Apple"].sort_values(by=["date"], ascending=False)
samsung = df[df["brand"]=="Samsung"].sort_values(by=["date"], ascending=False)
xiaomi = df[df["brand"]=="Xiaomi"].sort_values(by=["date"], ascending=False)

from wordcloud import WordCloud, ImageColorGenerator

# Define a function to create a wordcloud from dictionary of word frequency
def wordcloud_from_frequency(word_freq_dict, title, figure_size=(10, 6)):
    wordcloud.generate_from_frequencies(word_freq_dict)
    plt.figure(figsize=figure_size)
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.title(title)
    plt.show()

# Define a function to plot top10 positive words and top10 negative words in a grouped bar plot (from dictionaries)
def topn_wordfreq_bar_both(pos_word_freq_dict, neg_word_freq_dict, pos_num_doc, neg_num_doc, topn, title, palette, height=6, aspect=2):
    # Transform positive word frequency into DF
    df_pos = pd.DataFrame.from_dict(pos_word_freq_dict, orient="index").sort_values(by=0, ascending=False).head(topn)
    df_pos.columns = ["frequency"]
    df_pos["frequency"] = df_pos["frequency"] / pos_num_doc
    df_pos["label"] = "Positive"
    # Transform negative word frequency into DF
    df_neg = pd.DataFrame.from_dict(neg_word_freq_dict, orient="index").sort_values(by=0, ascending=False).head(topn)
    df_neg.columns = ["frequency"]
    df_neg["frequency"] = df_neg["frequency"] / neg_num_doc
    df_neg["label"] = "Negative"
    # Append two dataframes
    df_append = df_pos.append(df_neg)
    df_append.reset_index(inplace=True)
    # Plot
    sns.catplot(x="index", y="frequency", hue="label", data=df_append,
                kind="bar",
                palette=palette,
                height=height, aspect=aspect,
                legend_out=False)
    plt.title(title)
    plt.show()

apple_pos = " ".join(apple[apple["positivity"]==1]["clean_text"][0:1000])
apple_pos_word_freq = word_freq_dict(apple_pos)
wordcloud = WordCloud(width=5000,
                      height=3000,
                      max_words=200,
                      colormap="Blues",
                      background_color="white")
wordcloud_from_frequency(apple_pos_word_freq, "Most Frequent Words in the Latest 1000 Positive Reviews for Apple")

df["positivity"] = df["rating"].apply(lambda x: 1 if x>3 else(0 if x==3 else -1))

samsung_pos = " ".join(samsung[samsung["positivity"]==1]["clean_text"][0:10])
samsung_pos_word_freq = word_freq_dict(samsung_pos)
wordcloud = WordCloud(width=5000,
                      height=3000,
                      max_words=200,
                      colormap="Greens",
                      background_color="white")
wordcloud_from_frequency(samsung_pos_word_freq, "Most Frequent Words in the Latest 10 Positive Reviews for Samsung")

samsung[samsung["clean_text"].apply(lambda x: "new" in x)]["title"].value_counts().sort_values(ascending=True).tail(10).plot(kind="barh")
plt.title("Most reviews that mention 'new' are from renewed samsung buyers")
plt.show()

samsung["renewed"] = samsung["title"].apply(lambda x: ("Renewed" in x) | ("Reburshied" in x))
print("{0: 0.1%} samsung that were sold on flipkart are renewed/reburshied.".format(samsung["renewed"].sum() / len(samsung["renewed"])))

samsung_neg = " ".join(samsung[samsung["positivity"]==-1]["clean_text"][0:1000])
samsung_neg_word_freq = word_freq_dict(samsung_neg)
wordcloud = WordCloud(width=5000,
                      height=3000,
                      max_words=200,
                      colormap="Blues",
                      background_color="black")
wordcloud_from_frequency(samsung_neg_word_freq, "Most Frequent Words in the Latest  Negative Reviews for samsung")

topn_wordfreq_bar_both(samsung_pos_word_freq, samsung_neg_word_freq,
                       min(sum(samsung["positivity"]==1), 1000),
                       min(sum(samsung["positivity"]==-1), 1000),
                       10,
                       "Top10 Frequent Words in Latest Positive and Negative Reviews for samsung",
                       ["lightblue", "lightcoral"],
                       height=6, aspect=2)

import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
n_topics=10
lda = LatentDirichletAllocation(n_components=n_topics,
                                max_iter=50,
                                learning_method='online',
                                learning_offset=50.,
                                random_state=0)
def print_topn_words(model, feature_names, topn):
    for topic_idx, topic in enumerate(model.components_):
        message = "Topic #%d: " % topic_idx
        message += " ".join([feature_names[i]
                             for i in topic.argsort()[:-topn - 1:-1]])
        print(message)
    print()
tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.05, stop_words="english")
t0 = time.time()
samsung_tfidf = tfidf_vectorizer.fit_transform(samsung["clean_text"])
samsung_tfidf_feature_names = tfidf_vectorizer.get_feature_names()
lda.fit(samsung_tfidf)
print("Below is the output from LDA model with {} topics (each includes Top10 words) for Samsung.".format(n_topics))
print_topn_words(lda, samsung_tfidf_feature_names, 10)
print("Done in %0.3fs." % (time.time() - t0))

import xgboost as xgb
xgb_clf = xgb.XGBClassifier()
xgb_clf.fit(samsung_tfidf, samsung["positivity"])
featureImport = pd.DataFrame(xgb_clf.feature_importances_, index=samsung_tfidf_feature_names)
featureImport.columns = ["Importance"]
featureImport.sort_values(["Importance"], ascending=True).tail(20).plot(kind="barh", figsize=(10, 6))
plt.title("XGBoost Relative Feature Importance (from all reviews for samsung)")
plt.show()

from sklearn.feature_extraction.text import CountVectorizer
bow_samsung = CountVectorizer(analyzer = clean_text, ngram_range=(1,2)).fit(samsung['clean_text'])
bow_samsung = bow_samsung.transform(samsung['clean_text'])

from sklearn.feature_extraction.text import TfidfTransformer
tfidf_samsung = TfidfTransformer().fit(bow_samsung)
tfidf_samsung = tfidf_samsung.transform(bow_samsung)

from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB().fit(tfidf_samsung,samsung['positivity'])
predict = model.predict(tfidf_samsung)
from sklearn.metrics import classification_report,confusion_matrix
print(classification_report(samsung['positivity'],predict))

tn, fp, fn, tp = confusion_matrix(samsung['positivity'],predict).ravel()

from sklearn import svm

clf_svm = svm.SVC(kernel='linear')
clf_svm.fit(tfidf_samsung,samsung['positivity'])
clf_svm.predict(tfidf_samsung)
from sklearn.metrics import classification_report,confusion_matrix
print(classification_report(samsung['positivity'],predict))

from sklearn.ensemble import RandomForestClassifier

clf_rf = RandomForestClassifier(n_estimators=500)
clf_rf.fit(tfidf_samsung,samsung['positivity'])
clf_rf.predict(tfidf_samsung)
from sklearn.metrics import classification_report,confusion_matrix
print(classification_report(samsung['positivity'],predict))

X = samsung['clean_text']
y = samsung['positivity']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=101, stratify=y)

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier

pipeline = Pipeline([
    # tokenizing (counting words)
    ('bow',CountVectorizer(analyzer=clean_text)),
    # TF-IDF Scores (pembobotan)
    ('tfidf',TfidfTransformer()),

    ('classification', RandomForestClassifier())
])

pipeline.fit(X_train,y_train)

predict = pipeline.predict(X_test)
print(classification_report(y_test,predict))

pred_proba = pipeline.predict_proba(X_test)
pred_proba

predict = [ 1 if prob > 0.60 else -1 for prob in pred_proba[:,1]]
print(classification_report(y_test,predict))

len(stopwords.words('english'))

!pip install langdetect
from langdetect import detect
def lang_detect(text):
    try:
               return detect(text)
    except:
         return None
import time
start_time = time.time()
df["lang"] = df["subject"].apply(lang_detect)
print("It takes %s seconds for the code to finish." % (time.time() - start_time))

df["lang"].value_counts()[:10].plot(kind="barh", title="Number of Reviews grouped by Top10 Language")
plt.show()

df = df[df["lang"]=="en"]

import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()
analyzer.polarity_scores("mobile looks great.")

start_time = time.time()
df["subject"] = df["subject"].astype("str")
df["sent_neg"] = df["subject"].apply(lambda x: analyzer.polarity_scores(x)["neg"])
df["sent_neu"] = df["subject"].apply(lambda x: analyzer.polarity_scores(x)["neu"])
df["sent_pos"] = df["subject"].apply(lambda x: analyzer.polarity_scores(x)["pos"])
df["sent_comp"] = df["subject"].apply(lambda x: analyzer.polarity_scores(x)["compound"])
print("It takes %s seconds for the code to finish." % (time.time() - start_time))

df.to_csv("df_with_sentiment_scores.csv")

df_en = pd.read_csv("/content/df_with_sentiment_scores.csv")

plt.figure()

plt.subplot(2, 2, 1)
df_en["sent_neg"].hist(figsize=(10, 8), color="lightblue")
plt.title("Negative Sentiment Score")
plt.subplot(2, 2, 2)
df_en["sent_neu"].hist(figsize=(10, 8), color="grey")
plt.title("Neutral Sentiment Score")
plt.subplot(2, 2, 3)
df_en["sent_pos"].hist(figsize=(10, 8), color="lightgreen")
plt.title("Positive Sentiment Score")
plt.subplot(2, 2, 4)
df_en["sent_comp"].hist(figsize=(10, 8), color="lightcoral")
plt.title("Compound Sentiment Score")

plt.suptitle('Sentiment Analysis of Amazom Cell Phone Reviews', fontsize=12, fontweight='bold');

plt.show()

import numpy as np
import scipy.stats as stats
print("The correlation coefficient between sentiment score (compound) and rating is {0[0]: .4f} with a p-value of {0[1]: .4f}.".format(stats.pearsonr(df_en["rating"], df_en["sent_comp"])))
df_en.groupby("rating").mean()["sent_comp"].plot(kind="bar", figsize=(10, 6))
plt.title("Avg. Sentiment Score (Compound) per Rating")
plt.show()

import numpy as np
import scipy.stats as stats
print("The correlation coefficient between sentiment score (compound) and rating is {0[0]: .4f} with a p-value of {0[1]: .4f}.".format(stats.pearsonr(df_en["totalReviews"], df_en["sent_comp"])))
df_en.groupby("totalReviews").mean()["sent_comp"].plot(kind="bar", figsize=(10, 6))
plt.title("Avg. Sentiment Score (Compound) per Rating")
plt.show()